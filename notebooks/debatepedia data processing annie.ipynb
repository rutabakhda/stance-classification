{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from itertools import compress\n",
    "import time\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../data/debatepedia/debatepedia-preprocessed.json\", orient = \"records\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df =  df.drop_duplicates(subset=\"content\",keep=\"first\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = df.topic.unique()\n",
    "n = len(topics)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t = math.floor(n*0.8)\n",
    "n_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_tr = topics[:n_t]\n",
    "topics_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics_te = topics[n_t:]\n",
    "topics_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tr = df[df['topic'].isin(topics_tr)]\n",
    "df_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_te = df[df['topic'].isin(topics_te)]\n",
    "df_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr.to_json(\"../data/debatepedia/k-fold/set1/debatepedia-preprocessed-train.json\", orient = \"records\")\n",
    "df_te.to_json(\"../data/debatepedia/k-fold/set1/debatepedia-preprocessed-test.json\", orient = \"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_kfold(num_fold,index,split_path):\n",
    "    \"\"\"\n",
    "    Split folders into 80% training and 20% testing based on index\n",
    "    \"\"\"\n",
    "    # walk through folders\n",
    "    folders = []\n",
    "    for entry in os.scandir(path):\n",
    "        if entry.is_dir():\n",
    "            folders.append(entry.path)\n",
    "    for f in folders:\n",
    "        print(f)\n",
    "    test_ratio = 1/num_fold\n",
    "    test_len = int(len(folders) * test_ratio)\n",
    "    t1= int(len(folders) * test_ratio * (index-1))\n",
    "    t2 = t1+test_len\n",
    "    # split test set according to t1 and t2\n",
    "    train = folders[:t1] + folders[t2:]\n",
    "    test = folders[t1:t2]\n",
    "    parent = os.path.join(split_path,\"set\"+str(index))\n",
    "    \n",
    "    # create train path and test path\n",
    "    train_path = os.path.join(parent, \"train\")\n",
    "    test_path = os.path.join(parent, \"test\")\n",
    "    print(\"train path: \" + train_path)\n",
    "    print(\"test path: \" + test_path)\n",
    "\n",
    "    # copy the folders to train path or test path\n",
    "    # according to the split\n",
    "\n",
    "    print(len(train))\n",
    "    print(len(test))\n",
    "    for f in train:\n",
    "        folder_name = f.split(\"/\")[-1]\n",
    "        write_path = os.path.join(train_path, folder_name)\n",
    "        copy_folder(f, write_path)\n",
    "    for f in test:\n",
    "        folder_name = f.split(\"/\")[-1]\n",
    "        write_path = os.path.join(test_path, folder_name)\n",
    "        copy_folder(f, write_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_folder(src, des):\n",
    "    print(src)\n",
    "    print(des)\n",
    "    try:\n",
    "        shutil.copytree(src, des)\n",
    "        # Directories are the same\n",
    "    except shutil.Error as e:\n",
    "        print('Directory not copied. Error: %s' % e)\n",
    "        # Any error saying that the directory doesn't exist\n",
    "    except OSError as e:\n",
    "        print('Directory not copied. Error: %s' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"../data/debatepedia/xmi\"\n",
    "num_fold = 5\n",
    "#### Write to files\n",
    "t = datetime.now()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt = str(t)[:19].replace(' ', '_')\n",
    "parent = str(Path(path).parent)\n",
    "split_path = os.path.join(parent, dt)\n",
    "if not os.path.exists(split_path):\n",
    "    os.mkdir(split_path)\n",
    "print(split_path)\n",
    "for i in range(1,num_fold+1):\n",
    "    split_kfold(num_fold,i,split_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts to run the whole pipeline \n",
    "from reading json file, generating xmi, splitting, generating arff to evaluation by Weka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/adu_classification.sh\n",
    "\n",
    "# ADU Classification\n",
    "#./scripts/adu_classification.sh >&1 | tee  \"output/$(date +\"%Y-%m-%d_%T\").log\"\n",
    "\n",
    "# ADU 5 fold Validation\n",
    "#./scripts/adu_classification.sh kfold >&1 | tee  \"output/$(date +\"%Y-%m-%d_%T\").log\"\n",
    "\n",
    "# ADU Random Split Classification\n",
    "#./scripts/adu_classification.sh random >&1 | tee  \"output/$(date +\"%Y-%m-%d_%T\").log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADU classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### output from output/2019-05-31_03:42:29.log\n",
    "\n",
    "Step 1: Split File into Training & Testing\n",
    "/home/ciso0478/wstud-visit-the-dome-ss19/data/debatepedia/2019-05-31_03:42:29\n",
    "....ok\n",
    "\n",
    "Step 2: Use UIMA to convert to XMI files\n",
    "input directory: /home/ciso0478/wstud-visit-the-dome-ss19/data/debatepedia/2019-05-31_03:42:29\n",
    "filename: debatepedia-preprocessed_train.json\n",
    "output directory: /home/ciso0478/wstud-visit-the-dome-ss19/data/debatepedia/2019-05-31_03:42:29/xmi/debatepedia-preprocessed_train\n",
    ".....................................................................................................................................................................................................................................................................................................................................................................................................filename: debatepedia-preprocessed_test.json\n",
    "output directory: /home/ciso0478/wstud-visit-the-dome-ss19/data/debatepedia/2019-05-31_03:42:29/xmi/debatepedia-preprocessed_test\n",
    "...........................................................................done\n",
    "....ok\n",
    "\n",
    "Step 3: Generate Feature Files\n",
    "---------------------------------------------\n",
    "Processing corpus in the directory \n",
    "/home/ciso0478/wstud-visit-the-dome-ss19/data/debatepedia/2019-05-31_03:42:29/xmi/debatepedia-preprocessed_train\n",
    "---------------------------------------------\n",
    "\n",
    "Compute feature values on /home/ciso0478/wstud-visit-the-dome-ss19/data/debatepedia/2019-05-31_03:42:29/xmi/debatepedia-preprocessed_train\n",
    "finished in 21.751s\n",
    "\n",
    "---------------------------------------------\n",
    "Processing corpus in the directory \n",
    "/home/ciso0478/wstud-visit-the-dome-ss19/data/debatepedia/2019-05-31_03:42:29/xmi/debatepedia-preprocessed_test\n",
    "---------------------------------------------\n",
    "\n",
    "Compute feature values on /home/ciso0478/wstud-visit-the-dome-ss19/data/debatepedia/2019-05-31_03:42:29/xmi/debatepedia-preprocessed_test\n",
    "finished in 4.205s\n",
    "\n",
    "....ok\n",
    "\n",
    "Step 4: Use Weka to train classifier\n",
    "/home/ciso0478/wstud-visit-the-dome-ss19/data/debatepedia/2019-05-31_03:42:29\n",
    "\n",
    "Time taken to test model on training data: 18.36 seconds\n",
    "\n",
    "=== Error on training data ===\n",
    "\n",
    "Correctly Classified Instances       58624               99.9966 %\n",
    "Incorrectly Classified Instances         2                0.0034 %\n",
    "Kappa statistic                          0.9999\n",
    "Mean absolute error                      0.0274\n",
    "Root mean squared error                  0.0576\n",
    "Relative absolute error                  5.4715 %\n",
    "Root relative squared error             11.5226 %\n",
    "Total Number of Instances            58626     \n",
    "\n",
    "\n",
    "=== Detailed Accuracy By Class ===\n",
    "\n",
    "                 TP Rate  FP Rate  Precision  Recall   F-Measure  MCC      ROC Area  PRC Area  Class\n",
    "                 1.000    0.000    1.000      1.000    1.000      1.000    1.000     1.000     conclusion\n",
    "                 1.000    0.000    1.000      1.000    1.000      1.000    1.000     1.000     premise\n",
    "Weighted Avg.    1.000    0.000    1.000      1.000    1.000      1.000    1.000     1.000     \n",
    "\n",
    "\n",
    "=== Confusion Matrix ===\n",
    "\n",
    "     a     b   <-- classified as\n",
    " 29313     0 |     a = conclusion\n",
    "     2 29311 |     b = premise\n",
    "\n",
    "Time taken to test model on test data: 2.6 seconds\n",
    "\n",
    "=== Error on test data ===\n",
    "\n",
    "Correctly Classified Instances        6916               91.4815 %\n",
    "Incorrectly Classified Instances       644                8.5185 %\n",
    "Kappa statistic                          0.7704\n",
    "Mean absolute error                      0.156 \n",
    "Root mean squared error                  0.2552\n",
    "Relative absolute error                 31.2096 %\n",
    "Root relative squared error             51.036  %\n",
    "Total Number of Instances             7560     \n",
    "\n",
    "\n",
    "=== Detailed Accuracy By Class ===\n",
    "\n",
    "                 TP Rate  FP Rate  Precision  Recall   F-Measure  MCC      ROC Area  PRC Area  Class\n",
    "                 0.794    0.044    0.862      0.794    0.827      0.772    0.962     0.912     conclusion\n",
    "                 0.956    0.206    0.931      0.956    0.944      0.772    0.962     0.984     premise\n",
    "Weighted Avg.    0.915    0.164    0.913      0.915    0.914      0.772    0.962     0.966     \n",
    "\n",
    "\n",
    "=== Confusion Matrix ===\n",
    "\n",
    "    a    b   <-- classified as\n",
    " 1537  398 |    a = conclusion\n",
    "  246 5379 |    b = premise\n",
    "\n",
    "....ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADU 5 fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output from output/2019-05-31_17:00:22.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary of cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1=90.5621\n",
    "set2=91.4556\n",
    "set3=90.6409\n",
    "set4=90.5261\n",
    "set5=90.8758"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adu random classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "91.4919"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
